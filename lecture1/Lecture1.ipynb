{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a1414ad-6c0d-4be9-af8b-a2625b2f6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tinygrad import Tensor\n",
    "\n",
    "# setting tinygrad device to gpu since METAL is broken on older macs\n",
    "os.environ['GPU'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3321af30-93fc-4e2f-8d30-02d4f2b7fc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 6 6 4]]\n",
      "[[1.322044e-15]]\n"
     ]
    }
   ],
   "source": [
    "# perceptron\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size):\n",
    "        # init inputs, weights and bias\n",
    "        self.inputs = Tensor.randint(1, input_size)\n",
    "        self.weights = Tensor.randn(1, input_size)\n",
    "        self.bias = Tensor.randn(1,1)\n",
    "\n",
    "    def forward(self):\n",
    "        \n",
    "        # perform forward perceptron computation\n",
    "\n",
    "        # take sum of dot prod. of inputs (transposed) and their resp. weight\n",
    "        # we take sum because we do an element wise op\n",
    "        output = (self.inputs.T).dot(self.weights).sum()\n",
    "\n",
    "        # add bias\n",
    "        output = output.add(self.bias)\n",
    "        \n",
    "        # apply non-linear activation func.\n",
    "        output = output.sigmoid()\n",
    "        return output\n",
    "\n",
    "# Spawn perceptron with input size 4\n",
    "p1 = Perceptron(4)\n",
    "\n",
    "# Forward pass\n",
    "output = p1.forward()\n",
    "\n",
    "print(p1.inputs.numpy())\n",
    "print(output.numpy())\n",
    "\n",
    "# see how we 'squash' the 4 input nodes to 1 output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f961b70f-6344-4b45-a0b1-ad75f50729a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -8   3  -3  -3  -6  -5   2 -10]]\n",
      "[[9.9993065e-16 8.8749919e-03 3.4108248e-02 3.3258490e-07]]\n"
     ]
    }
   ],
   "source": [
    "# Multi Output Perceptron\n",
    "\n",
    "class MOP:\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        # init inputs, weights and bias\n",
    "        self.inputs = Tensor.randint(1, num_inputs, low=-10, high=10)\n",
    "        \n",
    "        # each node now has multiple outputs, so we need to init multiple weights per output\n",
    "        self.weights = Tensor.randn(num_inputs, num_outputs)\n",
    "        self.bias = Tensor.randn(1,1)\n",
    "\n",
    "    def forward(self):\n",
    "        # perform forward MLP computation\n",
    "        # we now need a matmul since in general, we cannot dot with matrices\n",
    "\n",
    "        # matmul of input and their resp. weight\n",
    "        # since we do a complex op matmul, we do not need to use sum, innate\n",
    "        # we also do not need to transpose with matmul\n",
    "        # otherwise we could do the same as the perceptron class, and do the transposed dot and sum\n",
    "\n",
    "        output = (self.inputs).matmul(self.weights)\n",
    "\n",
    "        # add bias\n",
    "        output = output.add(self.bias)\n",
    "        \n",
    "        # apply non-linear activation func.\n",
    "        output = output.sigmoid()\n",
    "        return output\n",
    "\n",
    "# Dense layer\n",
    "# ( all inputs are densely connected to all outputs )\n",
    "# ( so now we take out inputs, and 'squash' them into <1 outputs )\n",
    "\n",
    "\n",
    "# Spawn dense layer with 3 inputs and 2 outputs\n",
    "p1 = MOP(8,4)\n",
    "\n",
    "\n",
    "# forward pass\n",
    "output = p1.forward()\n",
    "\n",
    "print(p1.inputs.numpy())\n",
    "print(output.numpy())\n",
    "\n",
    "# see how we 'squash' the 3 input nodes to 2 output nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4420178b-8dc8-4f80-89e2-c51045af56b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 4 9]]\n",
      "[[6.5402872e-07 8.8858127e-07 9.9999714e-01 4.3373516e-01]]\n",
      "[[0.90946215 0.8645813 ]]\n"
     ]
    }
   ],
   "source": [
    "# single layer neural network\n",
    "# 3 -> 4 -> 2\n",
    "# num of nodes\n",
    "\n",
    "# input\n",
    "input_layer = MOP(3,4)\n",
    "print(input_layer.inputs.numpy())\n",
    "\n",
    "# hidden layer\n",
    "hidden_layer = MOP(input_layer.forward().size(dim=1), 2)\n",
    "hidden_layer.inputs = input_layer.forward()\n",
    "print(hidden_layer.inputs.numpy())\n",
    "\n",
    "# output\n",
    "output_layer = hidden_layer.forward()\n",
    "print(output_layer.numpy())\n",
    "\n",
    "# see how we went from 3 input nodes to 4 hidden layer nodes to 2 output nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44de71f-5ad3-4862-ae61-9e99d1803220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
